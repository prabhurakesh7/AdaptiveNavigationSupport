train stanford ner big gazette memory issue 
previously trained german classifier stanford ner training file tokens classes hours cut lot features short prop file gazette file unique tagged tokens retrain classifier tokens running memory issues gazette txt 386mb token objects unique reduced amount classes reduced amount tokens gazette million removed features listed stanford ner faq site prop file run memory heap space error 16gb ram start jvm mx15g xmx14g error occurs hours process problem dont reduce memory usage arbitrarily deleting entries gazette suggestions reduce memory usage prop file isnt troublesome advance 

regexner href http nlp stanford static software regexner rel nofollow http nlp stanford static software regexner thoughts ol li start entries big gazetteer handle large shrink li li sort entries frequent large corpus eliminate infrequent li li lot rarer entries gazetteer arent ambiguous regexner rule based layer system automatically tags person li ol 