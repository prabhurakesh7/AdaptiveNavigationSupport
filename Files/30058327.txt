sequential implementation times faster parallel implementation 
created simple scenario recognized weird behavior understand link created sequential implementation href http ideone b8jyea rel nofollow http ideone b8jyea basically big arrays fixed size algorithm iterates run workstation takes seconds implemented parallel version threads run simultaneously code thread dependency threads code runs times slower workstation href http ideone yfwvmr rel nofollow http ideone yfwvmr idea edit problem differs reason caching problem solve caching problem 

biggest overhead time spent starting stopping threads reduce size array takes amount time thread pool divide work thread write local data set 4x faster machine cores sequentialimplementation sec parallelimplementationoptimised sec hr writing data cache line means data pass l3 cache miss takes 20x longer access l1 cache suggest completely separate data structures bytes touching cache line intended complete overwrite cache line x64 cpus pull previous values cache line question blockquote isnt 20x slower blockquote cpu core grabbed cache line threads running hyper threading threads access data locally cpu loop times loses cache line cpu core demanding means 20x penalty access loop slower result 