control distribution tasks spark 
distributing download tasks spark cluster input source parallelized sparks normal methods service providing bunch download tasks url encapsulated logic read decrypt distribute thousands tasks spark distributes tasks equally slaves enabling maximum level parallelism oly hundred tasks spark thinks dataset tiny computed slaves reduce communication time increase data locality wrong case task produce thousands json records downloads performed machines cluster ideas moment ul li set number partitions number cores li li set number partitions number download tasks li ul dont pass number cores piece code resources spark job running time future pass number cores divided number parallel jobs run cluster dont partitioning thousands partition nodes awkward spark distribute elements rdd options preferrable 

download produce lot records dont lot downloads thousand recommend creating strong partition download strong total overhead scheduling thousand tasks seconds routinely tens thousands partitions production downloads partition end large partitions partition fit memory entirety issues operations build hash tables entire partition hr shouldnt takes parameter number partitions list urls large amount data create rdd number partitions begin shuffling creation 