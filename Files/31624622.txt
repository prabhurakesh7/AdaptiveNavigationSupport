change replication factor rdds spark 
understand multiple copies data rdds cluster case failure node program recover cases chance failure negligible costly memory wise multiple copies data rdds question parameter spark reduce replication factor rdds 

spark automatically cache simply applications create reused call set storage level persist shorthand default storage level scala differs creating refer constructor api doc youre python href https spark apache org docs latest programming guide html rdd persistence rel nofollow doc details number storage levels theyre fundamentally configuration shorthand point spark object extends href https github apache spark blob master core src main scala org apache spark storage storagelevel scala rel nofollow class define replication factor predefined storage levels single copy fact true isnt postfixed ul li disk li li memory li li memory ser li li memory disk li li memory disk ser li li heap li ul copy medium employ single copy choose single medium storage level 