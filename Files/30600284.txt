partial derivatives back propagation hidden layer 
yesterday posted question href http stackoverflow questions programming back propagation algorithm piece back propagation aglorithm today working understand hidden layer lot questions read websites papers subject matter read hard time applying actual code code analyzing working nice real problem dont methods work code neurons hidden layer connection neuron hidden layer grabs connections output summation incoming connections run sig function connection weight double ai input connection values node sum input neuron loop pretty sums neu getconnection id getweight dont understand desired output desiredoutput final layer node ak actual output summation activation function node summation activation weight strong edit strong started working code public class backprop 

standard backpropagation algorithm backpropagating error hidden layers output layer error neuron hidden layer dependent succeeding layer lets assume neuron em em synapses connect neurons em em em em em em layer assume output neuron em em em em error neuron em em equal expression assuming logistic function activation function em em em em em em em ai em em aj em em ak em em em em em derivative activation function em em error neuron em em em ai em weight assigned synapse connection em em em em applies remaining terms notice taking account error em em neuron em em layer em em connected notice taking account weight accorded synapse math makes sense intuitively error em em dependent errors neuron em em connects dependent weights synapses connections em em neurons layer errors update weights synapses connections neuron em previous em layer strong connects em em strong strong backpropagate strong error assume single neuron em em connects em em adjust em za em em za em em za em em em em em neurons previous layer connect em em update weights formula code happening neuron hidden layer ul li list synapses connections connect em em neuron em previous em layer part li li connection code ul li output neuron em em term formulas li li output neuron connects em em neuron em em term li li neuron output layer calculates sum error output neuron times weight neuron hidden layer neuron output layer expression em ai em em aj em em ak em em em calculate error output layer simply multiply derivative activation function output layer neuron difference actual expected output finally multiply thing em wjk em em ai em term formula li li values plug formula adjust weights synapse connects neuron preceding layer problem code calculates things differently ul li formula em em em em em ai em em aj em em ak em error neuron em em code calculates including terms equivalent em em em em em em em ai em em aj em em ak em mathematically works end multiplying learning rate em em em em difference code performs multiplication em em earlier li li calculates em em em em formula code li li update weight adding delta current weight em za em em em em em li ul li li things code doesnt set weight directly deals add fraction previous delta weight technique neural networks ensure network doesnt stuck local minima momentum term push local minima error surface neural network traversing error surface find lowest error stuck isnt deep optimal solution ensures converge solution careful set high overshoot optimal solution calculates weight momentum sets connection synapse li ul li ul hope explanation made clearer math hard figure makes sense main problem code written slightly manner code href https github vivin digitrecognizingneuralnetwork blob master src main net vivin neural backpropagator rel nofollow wrote implements backpropagation algorithm class project runs pretty lines formulas follow easily href https www youtube watch yldbwdgwiku rel nofollow video made explain backpropagation algorithm 