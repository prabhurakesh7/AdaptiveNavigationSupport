custom unbounded sources work google cloud dataflow 
implement custom unbounded source google cloud dataflow read amazon kinesis queue order implement checkpointing properly id understand mechanism works h1 dataflow works h1 understand checkpoints reading documentation dataflow crucial things missing millwheel paper explain understood concept laid paper focus interaction source consumer strong production setup terms dataflow api ul li called source null passed checkpointmark li li called reader instance li li called times reader li li worker decides make checkpoint mark calls reader li li checkpoint persisted worker li li called checkpoint object li li data read consumer stores records cache order deduplicate retries li li consumer sends ack source point checkpoint removed source ack accepted consumer removes records cache source wont retry point li li source fails receive ack create reader instance passing checkpoint argument retry sending data consumer consumer receives retried data deduplicate li li repeated unclear reader instance continue reading stream reader null checkpoint mark created order reader checkpoint data continue reading stream li ul h1 pubsub kinesis h1 words kinesis queue operates significant differences pub understand pub works havent h2 pub h2 pub pull model heavily relies acks messages received client acked internal checkpoint pub moves forward means upcoming pull request receive consecutive records previous ack h2 kinesis h2 kinesis pull interface push similar interact file start reading location stream special values trim horizon oldest record stream latest latest record stream move forward record record iterator iterators stored server side minutes expiry time unused acks server responsibility client track position stream read records expired h1 question issues h1 ul li checkpoint reader checkpoint expected read part data relates expected read data checkpoint words checkpoint data data li li reader null checkpoint mark perfectly fine means start reading point defined application developer dataflow create readers null id imagine situation reader jvm dies dataflow creates reader passing null checkpoint situation dont starting position read data previous reader mark progress lost li li id deduplication records consumer side returned question thought position stream unique stream happen join kinesis sources flattening lead situation records share id stream position tuple id unique case li ul cheers przemek 

excited dataflow kinesis love pull request href https github googlecloudplatform dataflowjavasdk rel nofollow github project href https github googlecloudplatform dataflowjavasdk blob master contrib readme md community contributions rel nofollow contrib connector kinesis happy review code github develop give feedback blockquote checkpoint reader checkpoint expected read part data relates expected read data checkpoint words checkpoint data data blockquote checkpoint mark represent data produced finalized reader reader responsible specific shard checkpoint mark consist shard identifier sequence number shard successfully read indicating data including produced blockquote reader null checkpoint mark perfectly fine means start reading point defined application developer dataflow create readers null id imagine situation reader jvm dies dataflow creates reader passing null checkpoint situation dont starting position read data previous reader mark progress lost blockquote finalized checkpoints persisted jvm failure words jvm dies reader constructed checkpoint finalized readers created null checkpoints intended read beginning source scenario jvm died successful call checkpoint mark reader construct iterator shard starts record read continue data loss blockquote id deduplication records consumer side returned getcurrentrecordid question thought position stream unique stream happen join kinesis sources flattening lead situation records share id stream position tuple id unique case blockquote dataflow unboundedsource implements overrides return de duped record ids required unique source instance records sources record ids treated duplicates flattening amazon kinesis guarantees records ids globally unique shards stream persistent resharding operations suitable record id em optional em method implement checkpointing scheme uniquely identifies record kinesis lets read records sequence number order sequence numbers globally unique assign shard worker worker produce duplicate data case worry record ids answer assumed simple case kinesis streams change shards hand sharding stream solution complex worker responsible shard checkpoint mark map shard sequence number sequence number split merged shards move dataflow workers balance load hard guarantee kinesis record read workers case kinesis record ids semantics suffice 