effective strategy avoid duplicate messages apache kafka consumer 
studying apache kafka month stuck point case consumer processes running machines ran tests published messages kafka server processing messages killed consumer processes restarted consumers writing processed messages file consumption finished file showing 10k messages messages duplicated consumer process disabled auto commit consumers manually commit offsets batch wise messages written file consumer commits offsets single consumer process running crashes recovers duplication avoided manner consumers running crashes recovers writes duplicate messages file effective strategy avoid duplicate messages 

short answer youre processing feasible relied caveats order attempt prevent duplicates simple consumer approach works consumer message consumed partition write partition offset consumed message disk consumer restarts failure read consumed offset partition disk pattern consumer guarantee wont reprocess message failure consumer consumes message fails offset flushed disk write disk process message write offset fail processing te message problem exist commit offsets zookeeper message cases processing attainable cases simply requires offset stored location unit applications output instance write consumer counts messages storing counted offset count guarantee offset stored time consumers state order guarantee processing require consume message update state message completely impractical kafka consumer applications nature kafka consumes messages batches performance reasons time spent application reliable simply design idempotent 