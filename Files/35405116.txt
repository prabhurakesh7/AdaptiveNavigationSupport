improving performance crawler4j 
write webscraper scrapes 1m websites saves title description keywords big file scraped url related words urls extracted big file ran crawler4j 1m urls file started webcrawler arbitrary number crawler passes resulted words blocking queue single thread write words url file writer thread order synchronize file set crawl depth crawl seed list running night downloaded 200k urls running scraper machine wired connection urls hosts dont politeness parameter importance edit starting crawler4j nonblocking start blocked crawler4j version code edit code mycrawler questions ol li suggest make process time tuning number crawler threads optimizations id prefer solution doesnt require machines role suggest code li li make crawler start working urls adding urls crawl looked doesnt work li ol advance 

default designed run machine field web crawler performance depends primary resources ul li disk li li cpu li li bandwidth li li ram li ul defining optimal number threads depends hardware setup machines result higher throughput hard limitation network bandwidth attached highspeed internet bottleneck approach designed load huge seed file default due fact resepcts crawler politness implies crawl starts seed point checked bit time adding seeds crawl started work crawl started blocking mode urls processed multi machine setup href http nutch apache org rel nofollow apache nutch nutch bit difficult learn strong edit strong reproducing setup answer issue addition seed pages dynamic starting crawler manner invoke method crawler thread investigating method find method named responsible taking unseen urls frontier order process method find called thread wait invoked controller shutdown threads reschedule urls overcome issue solutions ol li add url thread seed point deadlock situation occur starting threads blocking mode add seeds li li fork github project adapt code method invoked seed pages dynamically added li ol 