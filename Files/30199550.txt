achieve throughput large migration dynamodb 
running test large data migration dynamo intend prod account summer ran test batch write billion documents dynamo table hash range keys partial indexes document small 1k succeeded items written days disappointed dynamo performance experienced suggestions improve things order migration ec2 instances c4 8xlarges runs processes migration program weve split work processes internal parameters processes run longer process queries rds database records split partitions threadpool threads call dynamodb sdks batchsave method call batchsave sending documents 1k expect make single http call aws means time threads server making calls batchsave records rds instance handled load queries fine time ec2 instances ec2 side max cpu memory network network writes grouped hash key slow dynamo writes general group records split hash keys created dynamo table initially write throughput configured write throughput point test understanding partitions dynamo side handle variable responses times calls batchsave dynamo period span minutes running threads ec2 instance average time seconds median weve lot calls taking id expect consistency time takes make calls ec2 instance dynamo dynamo table plenty throughput configured ec2 instance cpu network healthy close maxed cloudwatch graphs console fairly terrible didnt show throttling write requests sample times processes finished work running threads ec2 instances happened dramatically improved response times calls dynamo running threads ec2 instance making calls batchsave response times improved 5x improved write throughput increased response times matter configured write throughput actual throughput exceed wed advice achieve performance dynamo migration production migration summer time sensitive migrate billion records advice achieve higher throughput rate pay units write throughput main index migration achieve performance close 

component batchwrite latency put request takes longest batch loop list dynamodbmapper failedbatch empty making progress fast running multiple parallel href http docs aws amazon awsjavasdk latest javadoc amazonaws services dynamodbv2 datamodeling dynamodbmapper html save 28t rel nofollow dynamodbmapper save calls batchsave make progress independently item write cloudwatch metrics minute metrics peaks consumption throttling masked minute window compounded fact sdk default retry throttled calls href https github aws aws sdk blob e925ba8defbb04a1c5f8af7860d4501186bbfeb7 aws sdk core src main amazonaws retry predefinedretrypolicies l112 rel nofollow times exposing provisionedthroughputexceededexception client making difficult pinpoint actual throttling happening improve understanding reducing number sdk retries request consumedcapacity total throttle writes guava ratelimiter href https awsblog post tx3vayqiz3q0zvw rate limited scans amazon dynamodb rel nofollow rate limited scan blog post log throttled primary keys patterns emerge finally number partitions table driven amount read write capacity units provision table driven amount data store table generally partition stores 10gb data split write table deleting entries number partitions table grow bound iops starvation provision wcu partitions due amount data 40k wcu distributed partitions average wcu partition control amount stale data table rate limited cleanup process scans removes entries href http www slideshare net amazonwebservices building applications dynamodb rel nofollow rolling time series tables slides delete migrate entire tables data relevant rolling time series tables expensive rate limited cleanup consume wcu deletetable operation consume wcu deleteitem call 