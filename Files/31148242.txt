encog neural net structure training data 
encog neural nets involved xor simple sentences word sentence type tag input layer inputs previous word current word previous word 1st input activated sentence word contingent previous word array similar xor dont load words sentences array id scan sentence time reach eof start back beginning super comfortable encog examples xor extremely complicated inputs input consists neurons chance word tag inputs neurons probability inputs arent activated neurons set output layer represents tags output neurons highest number tag chosen sentences word sentence inputs activate input demos encog networks trained single array holding training data looped network trained train network arrays array sentence format work 

neural net works word input activate input basic level run neural network problem ol li fixed length input vector feeding represented numerically fixed length entry vector single number li li set labels input vector correspond single fixed length output vector li ol neural net classifies edits close labels youre work words deep learning framework map words existing vector representation highly recommend href http nlp stanford projects glove rel nofollow glove href https code google word2vec rel nofollow word2vec decent learn top representation hr deeper understanding youre attempting issue youre dealing inputs inputs concatenation existing predictions words case word entries care mapping treat predict numbers numbers feel obliged youve framed problem awful performance dealing sparse zeros vector small dataset deep learning techniques show poor performance compared methods glove svm random forest model existing data 