upstream stream feeding multiple downstream streams 
general streams api problem id solve efficiently suppose possibly large possibly infinite stream pre process filtering items mutating lets assume pre processing complex time compute intensive distinct sets operations item sequence process end distinct sequence stream type construction infinite stream foreach finite collector collect intermediate results list drag separate streams list processing stream individually work finite stream ugly potentially impractical large stream flat wont work infinite stream guess peek kind tee perform chain processing results downstream peek coerce consumer peek work path longer stream discovered create blockingqueue peek push things queue obtain stream queue fine idea works fail understand stream closed heres sample code illustrating question question heck stream blockingqueue closed cheers toby 

interesting problem ill answer question simpler issue blockquote question heck stream blockingqueue closed blockquote closed stream number elements finishes disregarding elements added queue future reason stream queue represents strong current contents strong queue time stream created doesnt represent future elements thread add future stream represents current future contents queue technique href http stackoverflow answer basically call dont wont discuss larger issue source objects processing including filtering results send downstream processing steps essentially single producer consumers fundamental issues handle deal cases processing steps occur rates suppose weve solve issue stream queue stream terminating prematurely producer produce elements faster consumer process elements queue queue accumulate elements fills memory decide deal consumers processing elements rates consumer significantly slower arbitrary number elements buffered fill memory faster consumer slowed match average rate slower consumer toss sketch proceed dont actual requirements idea satisfactory thing parallel streams kind application problematic parallel streams dont deal blocking load balancing id start stream processing elements producer accumulating throws put put catch block write helper method caught queue fills block pipeline run sequentially thread parallel dedicated thread pool avoid blocking common pool consumers key handoff producer consumers batches method adds elements queue destination atomically empties queue consumers dont wait producer finish processing wont happen infinite addition consumers operating quantity data wont block midst processing consumer stream conceivably run parallel helpful consumers running lockstep consumers run rates youll construct additional queues buffer workloads independently consumers slower producer queue eventually fill blocked slowing producer rate consumers accept consumers faster producer average dont worry relative processing rates consumers loop pick producer managed put queue block outlined simplistic approach multi stage pipelining application performance critical find lot work tuning memory consumption load balancing increasing throughput reducing latency frameworks amenable application href https lmax exchange github io disruptor rel nofollow lmax disruptor dont experience 